<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Recovering from lost master hosts | Red Hat OpenShift on HPE SimpliVity</title>
    <meta name="description" content="">
    
    
    <link rel="preload" href="/OpenShift-on-SimpliVity/assets/css/0.styles.10caf491.css" as="style"><link rel="preload" href="/OpenShift-on-SimpliVity/assets/js/app.40e23d52.js" as="script"><link rel="preload" href="/OpenShift-on-SimpliVity/assets/js/2.1dbc2e0f.js" as="script"><link rel="preload" href="/OpenShift-on-SimpliVity/assets/js/5.b01a62a1.js" as="script"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/10.1f1d7f8e.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/11.11823b45.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/12.89cf306a.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/13.04e66138.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/14.117c52af.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/15.2b6725ef.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/16.f6c4226c.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/17.e2c26612.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/18.3f1906fb.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/19.c95e0f53.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/20.97be3970.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/21.b83682e9.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/22.2335fd4f.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/23.e7720519.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/24.81a433f1.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/25.bb8e22b0.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/26.501cade6.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/27.a7511174.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/28.5bc43de6.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/29.34bd11fa.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/3.50c5b2bc.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/30.bdf98bdc.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/31.75fe6739.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/32.337bc2fe.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/33.16f0a82e.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/34.aeee2b94.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/35.92a761c4.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/36.650b0ab6.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/37.8b242734.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/38.b80c9e2e.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/39.da81b7e9.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/4.e53f978e.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/40.ad9db5cb.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/41.f061e6b8.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/42.fbb640da.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/43.5f1e180f.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/44.20c11a66.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/45.d518ff7d.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/46.64ca7118.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/47.33e40d0c.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/48.f4633edd.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/49.c3f73c13.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/50.eefe5acf.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/51.10f537c8.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/52.c5e6db6a.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/53.b3f0289c.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/54.2f275aa2.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/55.1095b46e.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/56.6dae4490.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/6.e6477448.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/7.5c8997e8.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/8.cc524ee6.js"><link rel="prefetch" href="/OpenShift-on-SimpliVity/assets/js/9.f5f3bb57.js">
    <link rel="stylesheet" href="/OpenShift-on-SimpliVity/assets/css/0.styles.10caf491.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/OpenShift-on-SimpliVity/" class="home-link router-link-active"><!----> <span class="site-name">Red Hat OpenShift on HPE SimpliVity</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/OpenShift-on-SimpliVity/" class="nav-link">Home</a></div><div class="nav-item"><a href="/OpenShift-on-SimpliVity/blog/" class="nav-link">Blog</a></div> <a href="https://github.com/HewlettPackard/Openshift-on-Simplivity" target="_blank" rel="noopener noreferrer" class="repo-link">
    Contribute!
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/OpenShift-on-SimpliVity/" class="nav-link">Home</a></div><div class="nav-item"><a href="/OpenShift-on-SimpliVity/blog/" class="nav-link">Blog</a></div> <a href="https://github.com/HewlettPackard/Openshift-on-Simplivity" target="_blank" rel="noopener noreferrer" class="repo-link">
    Contribute!
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Introduction</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Release Notes</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution overview</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution components</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Preparing the environment</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Configuring the solution</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Overview of the playbooks</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Post deployment tasks</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Configuring storage</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Deploying worker nodes</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Deploying cluster logging</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Backup and restore</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/OpenShift-on-SimpliVity/backup-restore/backup-restore-intro.html" class="sidebar-link">Introduction to backup and restore</a></li><li><a href="/OpenShift-on-SimpliVity/backup-restore/backup.html" class="sidebar-link">Backup</a></li><li><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html" class="active sidebar-link">Recovering from lost master hosts</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#prerequisites" class="sidebar-link">Prerequisites</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#delete-2-master-nodes" class="sidebar-link">Delete 2 master nodes</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#restore-etcd-quorum-on-the-remaining-master" class="sidebar-link">Restore etcd quorum on the remaining master.</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#redeploy-two-master-nodes" class="sidebar-link">Redeploy two master nodes</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#grow-etcd-to-full-membership" class="sidebar-link">Grow etcd to full membership.</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#add-the-restored-master-hosts-to-the-etcd-cluster" class="sidebar-link">Add the restored master hosts to the etcd cluster</a></li><li class="sidebar-sub-header"><a href="/OpenShift-on-SimpliVity/backup-restore/recovery-lost-master.html#verify-master-host-has-been-added-to-the-etcd-member-list" class="sidebar-link">Verify master host has been added to the etcd member list</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Appendices</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="recovering-from-lost-master-hosts"><a href="#recovering-from-lost-master-hosts" aria-hidden="true" class="header-anchor">#</a> Recovering from lost master hosts</h1> <p>It is possible to use the etcd backup to recover from the scenario of one or more master nodes being lost. This includes situations where a majority of master hosts have been lost, leading to etcd quorum loss and the cluster going offline. This procedure assumes that you have at least one healthy master host.</p> <p>This procedure can be used to validate that your etcd backup has succeeded, but it is highly invasive it that it
requires you to destroy master nodes and would render your cluster unusable for the duration of the procedure.</p> <p>The information in this section is taken from <a href="https://docs.openshift.com/container-platform/4.1/backup_and_restore/disaster_recovery/scenario-1-infra-recovery.html" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.1/backup_and_restore/disaster_recovery/scenario-1-infra-recovery.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Please check the latest version of the OpenShift documentation for any updates to this procedure.</p> <h2 id="prerequisites"><a href="#prerequisites" aria-hidden="true" class="header-anchor">#</a> Prerequisites</h2> <ul><li>Ensure  that you can access the first master node <code>ocp-master0</code> usng <code>ssh</code></li> <li>Ensure that you have taken a backup, as outlined in the preceding section</li></ul> <div class="language- extra-class"><pre class="language-text"><code>ls -al  ~/backups/

-rw-rw-r--.  1 core core   625040 Sep 22 09:18 backup_2019_09_22_131749.misc.tgz
-rw-rw-r--.  1 core core 21732338 Sep 22 09:18 backup_2019_09_22_131749.snapshots.tgz
</code></pre></div><p>Unpack the snapshots:</p> <div class="language- extra-class"><pre class="language-text"><code>$ cd ~/backups
$ tar -xvf backup_2019_09_22_131749.snapshots.tgz

ocp-master1/
ocp-master0/
ocp-master2/
ocp-master1/assets/
ocp-master1/assets/backup/
ocp-master1/assets/backup/snapshot.db
ocp-master1/assets/backup/etcd-member.yaml
ocp-master1/assets/backup/etcd-client.key
ocp-master1/assets/backup/etcd-client.crt
ocp-master1/assets/backup/etcd-ca-bundle.crt
ocp-master0/assets/
ocp-master0/assets/backup/
ocp-master0/assets/backup/snapshot.db
ocp-master0/assets/backup/etcd-member.yaml
ocp-master0/assets/backup/etcd-client.key
ocp-master0/assets/backup/etcd-client.crt
ocp-master0/assets/backup/etcd-ca-bundle.crt
ocp-master2/assets/
ocp-master2/assets/backup/
ocp-master2/assets/backup/snapshot.db
ocp-master2/assets/backup/etcd-member.yaml
ocp-master2/assets/backup/etcd-client.key
ocp-master2/assets/backup/etcd-client.crt
ocp-master2/assets/backup/etcd-ca-bundle.crt
</code></pre></div><h2 id="delete-2-master-nodes"><a href="#delete-2-master-nodes" aria-hidden="true" class="header-anchor">#</a> Delete 2 master nodes</h2> <p>You can use the <code>playbooks/clean.yml</code> playbook to remove specific nodes, by setting the appropriate <code>limit</code>.
The following command will delete nodes <code>ocp-master1</code> and <code>ocp-master2</code>, but will leave <code>opc-master-0</code> running.</p> <div class="language- extra-class"><pre class="language-text"><code>ansible-playbook -i hosts playbooks/clean.yml --limit='master:!master[0]'
</code></pre></div><p>Now that the  majority of master hosts have been lost, etcd quorum will be lost and this will lead to the cluster going offline. If you try to run <code>oc</code> commands, they will not respond:</p> <div class="language- extra-class"><pre class="language-text"><code>oc get nodes
</code></pre></div><p>If you view the cluster health in the web console, you will see that the control plane status is unknown.</p> <p><img src="/OpenShift-on-SimpliVity/assets/img/cluster-health.b50d2961.png" alt=" &quot;Cluster health&quot;" title="Figure. Cluster health"></p> <p><strong>Figure. Cluster health</strong></p> <h2 id="restore-etcd-quorum-on-the-remaining-master"><a href="#restore-etcd-quorum-on-the-remaining-master" aria-hidden="true" class="header-anchor">#</a> Restore etcd quorum on the remaining master.</h2> <p>Copy a snapshot from the backup to the remaining master:</p> <div class="language- extra-class"><pre class="language-text"><code>scp ~/backups/ocp-master0/assets/backup/snapshot.db core@ocp-master0:~/
</code></pre></div><p>Connect to the reamining master <code>ocp-master0</code></p> <div class="language- extra-class"><pre class="language-text"><code>$ ssh  ocp-master0

Warning: Permanently added 'ocp-master0,10.15.155.210' (ECDSA) to the list of known hosts.
Red Hat Enterprise Linux CoreOS 410.8.20190830.0
WARNING: Direct SSH access to machines is not recommended.

---
Last login: Sun Sep 22 13:18:33 2019 from 10.15.155.7
[core@ocp-master0 ~]$

</code></pre></div><p>Set the <code>INITIAL_CLUSTER</code> variable to the single remaining member, in the format of <code>&lt;name&gt;=&lt;url&gt;</code>. First,
you need to determine the <code>name</code> and <code>url</code> to use:</p> <div class="language- extra-class"><pre class="language-text"><code>$ ETCDCTL=/var/home/core/assets/bin/etcdctl
$ ASSET_DIR=/home/core/assets/
$ sudo ETCDCTL_API=3 ${ETCDCTL} --cert $ASSET_DIR/backup/etcd-client.crt --key $ASSET_DIR/backup/etcd-client.key --cacert $ASSET_DIR/backup/etcd-ca-bundle.crt member list

1daf5dbed09ea2d3, started, etcd-member-ocp-master2, https://etcd-2.ocp.hpecloud.org:2380, https://10.15.155.212:2379
333583b05ff2cf8a, started, etcd-member-ocp-master0, https://etcd-0.ocp.hpecloud.org:2380, https://10.15.155.210:2379
4be0034d015274a2, started, etcd-member-ocp-master1, https://etcd-1.ocp.hpecloud.org:2380, https://10.15.155.211:2379
</code></pre></div><p>Now, set the environment variable using the information gathered for <code>ocp-master0</code>:</p> <div class="language- extra-class"><pre class="language-text"><code>export INITIAL_CLUSTER=&quot;etcd-member-ocp-master0.ocp.hpecloud.org=https://etcd-0.ocp.hpecloud.org:2380&quot;
</code></pre></div><p>Run the <code>etcd-snapshot-restore.sh</code> script, using the copied snapshot and the member list, in this case, just the <code>ocp-master0</code> member:</p> <div class="language- extra-class"><pre class="language-text"><code>sudo /usr/local/bin/etcd-snapshot-restore.sh /home/core/snapshot.db $INITIAL_CLUSTER


Downloading etcdctl binary..
etcdctl version: 3.3.10
API version: 3.3
etcd-member.yaml found in ./assets/backup/
Stopping all static pods..
..stopping etcd-member.yaml
..stopping kube-scheduler-pod.yaml
..stopping kube-controller-manager-pod.yaml
..stopping kube-apiserver-pod.yaml
Stopping etcd..
Waiting for etcd-member to stop
Stopping kubelet..
Stopping all containers..
019708c6aff244dbc47f90cec65c4823a93b8e3fe731f3a5e8f3cdedb0dc37ed
81eed61df3ee87bd960602f23e598ab48b43a6cb5610f4ecfed709f1e1b67119
...
Backing up etcd data-dir..
Removing etcd data-dir /var/lib/etcd
Restoring etcd member etcd-member-ocp-master0.ocp.hpecloud.org from snapshot..
2019-09-22 14:18:21.903011 I | pkg/netutil: resolving etcd-0.ocp.hpecloud.org:2380 to 10.15.155.210:2380
2019-09-22 14:18:22.154363 I | mvcc: restore compact to 713621
2019-09-22 14:18:22.207764 I | etcdserver/membership: added member 1c5a549c9f4e67b5 [https://etcd-0.ocp.hpecloud.org:2380] to cluster 11eeb64feb9a2071
Starting static pods..
..starting etcd-member.yaml
..starting kube-scheduler-pod.yaml
..starting kube-controller-manager-pod.yaml
..starting kube-apiserver-pod.yaml
Starting kubelet..
</code></pre></div><p>On your Ansible box, check the nodes in your cluster:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc get nodes
NAME          STATUS     ROLES    AGE   VERSION
ocp-master0   Ready      master   45h   v1.13.4+3bd346709
ocp-master1   NotReady   master   45h   v1.13.4+3bd346709
ocp-master2   NotReady   master   45h   v1.13.4+3bd346709
ocp-worker0   Ready      worker   45h   v1.13.4+3bd346709
ocp-worker1   Ready      worker   45h   v1.13.4+3bd346709
</code></pre></div><p>You need to explicitly delete the <code>NotReady</code> nodes <code>ocp-master1</code> and <code>ocp-master2</code>:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc delete node ocp-master1
$ oc delete node ocp-master2

$ oc get nodes

NAME          STATUS   ROLES    AGE   VERSION
ocp-master0   Ready    master   45h   v1.13.4+3bd346709
ocp-worker0   Ready    worker   45h   v1.13.4+3bd346709
ocp-worker1   Ready    worker   45h   v1.13.4+3bd346709
</code></pre></div><h2 id="redeploy-two-master-nodes"><a href="#redeploy-two-master-nodes" aria-hidden="true" class="header-anchor">#</a> Redeploy two master nodes</h2> <p>You need to re-build the nodes for <code>ocp-master1</code> and <code>ocp-master2</code>. You must ensure that your <code>/etc/hosts</code>, DNS, and load balancers are modified appropriately.</p> <p>If you have not made significant changes to
your cluster since deployment, you may be able to re-run <code>site.yml</code>:</p> <div class="language- extra-class"><pre class="language-text"><code># ansible-playbook -i hosts site.yml --vault-password-file .vault_pass
</code></pre></div><h2 id="grow-etcd-to-full-membership"><a href="#grow-etcd-to-full-membership" aria-hidden="true" class="header-anchor">#</a> Grow etcd to full membership.</h2> <p>Set up a temporary etcd certificate signer service on your master node that is an etcd member, in this case, <code>ocp-master0</code>. Connect to the master node and then log in to your cluster as a <code>cluster-admin</code> user using the following command:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc login https://localhost:6443

The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Authentication required for https://localhost:6443 (openshift)
Username: kubeadmin
Password:  LX65K-DXmpC-P4Hpo-W35au
Login successful.
</code></pre></div><p>Obtain the pull specification for the kube-etcd-signer-server image:</p> <div class="language- extra-class"><pre class="language-text"><code>export KUBE_ETCD_SIGNER_SERVER=$(sudo oc adm release info --image-for kube-etcd-signer-server --registry-config=/var/lib/kubelet/config.json --config=/home/core/.kube/config)
</code></pre></div><p>Run the tokenize-signer.sh script to generate three files:</p> <div class="language- extra-class"><pre class="language-text"><code>$ sudo -E /usr/local/bin/tokenize-signer.sh ocp-master0.ocp.hpecloud.org

Populating template /usr/local/share/openshift-recovery/template/kube-etcd-cert-signer.yaml.template
Populating template ./assets/tmp/kube-etcd-cert-signer.yaml.stage1
Tokenized template now ready: ./assets/manifests/kube-etcd-cert-signer.yaml
</code></pre></div><p>Use the generated <code>kube-etcd-cert-signer.yaml</code> file to deploy the signer pod:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc create -f assets/manifests/kube-etcd-cert-signer.yaml

pod/etcd-signer created
</code></pre></div><p>Verify that the signer is listening on this master node - it may take a minute or two to start up.</p> <div class="language- extra-class"><pre class="language-text"><code>$ ss -ltn | grep 9943
$ ss -ltn | grep 9943

LISTEN   0         128                       *:9943                   *:*
</code></pre></div><h2 id="add-the-restored-master-hosts-to-the-etcd-cluster"><a href="#add-the-restored-master-hosts-to-the-etcd-cluster" aria-hidden="true" class="header-anchor">#</a> Add the restored master hosts to the etcd cluster</h2> <p>Connect to one of the restored master nodes, in this case, <code>ocp-master1</code>:</p> <div class="language- extra-class"><pre class="language-text"><code># ssh ocp-master1
</code></pre></div><p>Log in to your cluster as a cluster-admin user using the following command:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc login https://localhost:6443

The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Authentication required for https://localhost:6443 (openshift)
Username: kubeadmin
Password:  LX65K-DXmpC-P4Hpo-W35au
Login successful.

</code></pre></div><p>Export two environment variables that are required by the etcd-member-recover.sh script:</p> <div class="language- extra-class"><pre class="language-text"><code>export SETUP_ETCD_ENVIRONMENT=$(sudo oc adm release info --image-for setup-etcd-environment --registry-config=/var/lib/kubelet/config.json  --config=/home/core/.kube/config)

export KUBE_CLIENT_AGENT=$(sudo oc adm release info --image-for kube-client-agent --registry-config=/var/lib/kubelet/config.json --config=/home/core/.kube/config)
</code></pre></div><p>Run the <code>etcd-member-recover.sh</code> script, passing in that IP address of <code>ocp-master0</code> and the name of the new etcd member:</p> <div class="language- extra-class"><pre class="language-text"><code>$ sudo -E /usr/local/bin/etcd-member-recover.sh 10.15.155.210 etcd-member-ocp-master1.ocp.hpecloud.org

Creating asset directory ./assets
Downloading etcdctl binary..
etcdctl version: 3.3.10
API version: 3.3
Backing up /etc/kubernetes/manifests/etcd-member.yaml to ./assets/backup/
Backing up /etc/etcd/etcd.conf to ./assets/backup/
Trying to backup etcd client certs..
etcd client certs found in /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8 backing up to ./assets/backup/
Stopping etcd..
Waiting for etcd-member to stop
Waiting for etcd-member to stop
Waiting for etcd-member to stop
Waiting for etcd-member to stop
Local etcd snapshot file not found, backup skipped..
Backing up etcd certificates..
Populating template /usr/local/share/openshift-recovery/template/etcd-generate-certs.yaml.template
Populating template ./assets/tmp/etcd-generate-certs.stage1
Populating template ./assets/tmp/etcd-generate-certs.stage2
Starting etcd client cert recovery agent..
Waiting for certs to generate..
Waiting for certs to generate..
Waiting for certs to generate..
Waiting for certs to generate..
Stopping cert recover..
Waiting for generate-certs to stop
Patching etcd-member manifest..
Updating etcd membership..
Member 81d77724154f987e added to cluster 11eeb64feb9a2071

ETCD_NAME=&quot;etcd-member-ocp-master1.ocp.hpecloud.org&quot;
ETCD_INITIAL_CLUSTER=&quot;etcd-member-ocp-master0=https://etcd-0.ocp.hpecloud.org:2380,etcd-member-ocp-master1.ocp.hpecloud.org=https://etcd-1.ocp.hpecloud.org:2380&quot;
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://etcd-1.ocp.hpecloud.org:2380&quot;
ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot;
Starting etcd..
</code></pre></div><h2 id="verify-master-host-has-been-added-to-the-etcd-member-list"><a href="#verify-master-host-has-been-added-to-the-etcd-member-list" aria-hidden="true" class="header-anchor">#</a> Verify master host has been added to the etcd member list</h2> <p>From your Ansible box, connect to the first master <code>ocp-master1</code>:</p> <div class="language- extra-class"><pre class="language-text"><code># ssh ocp-master0
</code></pre></div><p>Connect to the running etcd container:</p> <div class="language- extra-class"><pre class="language-text"><code>$ id=$(sudo crictl ps --name etcd-member | awk 'FNR==2{ print $1}')
$ sudo crictl exec -it $id /bin/sh

sh-4.2#
</code></pre></div><p>In the etcd container, export variables needed for connecting to etcd:</p> <div class="language- extra-class"><pre class="language-text"><code>sh-4.2# export ETCDCTL_API=3 
sh-4.2# export ETCDCTL_CACERT=/etc/ssl/etcd/ca.crt 
sh-4.2# export ETCDCTL_CERT=$(find /etc/ssl/ -name *peer*crt)                   
sh-4.2# export ETCDCTL_KEY=$(find /etc/ssl/ -name *peer*key)
</code></pre></div><p>In the etcd container, execute <code>etcdctl member list</code> and verify that the new member is listed.</p> <div class="language- extra-class"><pre class="language-text"><code>sh-4.2# etcdctl member list -w table
+------------------+---------+------------------------------------------+--------------------------------------+                               ----------------------------+
|        ID        | STATUS  |                   NAME                   |              PEER ADDRS              |                                       CLIENT ADDRS        |
+------------------+---------+------------------------------------------+--------------------------------------+                               ----------------------------+
| 1c5a549c9f4e67b5 | started |                  etcd-member-ocp-master0 | https://etcd-0.ocp.hpecloud.org:2380 |                                https://10.15.155.210:2379 |
| 81d77724154f987e | started | etcd-member-ocp-master1.ocp.hpecloud.org | https://etcd-1.ocp.hpecloud.org:2380 |                                https://10.15.155.211:2379 |
+------------------+---------+------------------------------------------+--------------------------------------+                               ----------------------------+
sh-4.2#
</code></pre></div><p>Note that it may take up to 10 minutes for the new member to start.</p> <p>Repeat these steps for <code>ocp-master2</code> until you have achieved full etcd membership.</p> <p>When you are finished resotring the etcd cluster, delete the signer pod ftrom the OpenShift cluster:</p> <div class="language- extra-class"><pre class="language-text"><code>$ oc delete pod -n openshift-config etcd-signer
</code></pre></div></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/Openshift-on-Simplivity/edit/master/docs-src/backup-restore/recovery-lost-master.md" target="_blank" rel="noopener noreferrer">Help us improve this page!</a> <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></div> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/OpenShift-on-SimpliVity/backup-restore/backup.html" class="prev">
          Backup
        </a></span> <span class="next"><a href="/OpenShift-on-SimpliVity/appendices/appendix-a.html">
          Appendix A: Bill of Materials
        </a>
        →
      </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/OpenShift-on-SimpliVity/assets/js/app.40e23d52.js" defer></script><script src="/OpenShift-on-SimpliVity/assets/js/2.1dbc2e0f.js" defer></script><script src="/OpenShift-on-SimpliVity/assets/js/5.b01a62a1.js" defer></script>
  </body>
</html>
