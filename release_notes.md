# Release Notes

## Bug Fixes

### Load balancers: firewall zone settings are lost after a reboot

- After a reboot, the two interfaces of the load balancer VMs are now assigned to the correct firewall zones.

### DNS and DHCP services are not automatically restarted after a reboot of a support node

- These services are now started automatically after a reboot.

## OCP 4.2 Support

### Support for OpenShift Container Platform version 4.2

- This version of the playbooks now supports the deployment of OCP 4.2.

### Scaling an OCP 4.2 cluster with Red Hat CoreOS worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).
- When an RHCOS worker node is added to the cluster, OCP 4.2 requires the approval of two Certificate Signing Requests (CSRs).
- The scale.yml playbook has been modified to approve these CSRs when adding RHCOS nodes to the cluster.

### Scaling an OCP 4.2 cluster with Red Hat 7.6 worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).
- Scaling an OCP 4.2 cluster with RHEL worker nodes requires the use of an updated version of the OpenShift Ansible playbooks https://github.com/openshift/openshift-ansible.
- HPE has tested with version 4.2.0-201910111434 of the OpenShift Ansible playbooks and recommends using this version when scaling an OCP 4.2 cluster with RHEL worker nodes.

### Make the master nodes non-schedulable

- By default, the OCP 4.2 installer generates Kubernetes manifests which will make the master nodes schedulable. This was done in an attempt to support clusters with smaller footprints.
- However, due to a limitation with Kubernetes where router Pods running on control plane machines will not be reachable by the ingress load balancer, a manifest generated by the installer needs to be modified prior to running the actual installation in order to make the master nodes non-schedulable.
- According to Red Hat, this may be corrected in a future minor version of OCP.

### EFK Logging Stack

- The process of deploying the EFK (Elasticsearch, Fluentd, Kibana) logging stack has changed slightly in OCP 4.2.
- The efk.yml playbook, and supporting efk role, have been updated to work with OCP 4.2.
- NOTE - This updated version of the EFK playbooks will not work with an OCP 4.1 cluster.

## New Features

### Sysdig Integration
You can now use the playbook `playbooks/sysdig.yml` to integrate your cluster with Sysdig. The implementation in this solution uses the Software as a Service (SaaS) version of Sysdig at [app.sysdigcloud.com](). The playbook deploys the Sysdig Agent software on all OpenShift node. Captured data is relayed back to your Sysdig SaaS Cloud portal.

Here are the variables that you must configure prior to using the `sysdig.yml` playbook.

| Variable                          | File                       | Description                                                  |
| --------------------------------- | -------------------------- | ------------------------------------------------------------ |
| `sysdig_access_key`               | `group_vars/all/vars.yml`  | Value must be `"{{ vault.sysdig_access_key }}"` if you want to encrypt the actual access key (recommended) |
| `vault.sysdig_access_key`         | `group_vars/all/vault.yml` | Your Sysdig access key                                       |
| `sysdig_agent_configmap_manifest` | `group_vars/all/vars.yml`  | Optional: See note 1 below                                   |

**note 1**:  The variable `sysdig_agent_configmap_manifest` is optional. The default value points to a manifest supplied by the `sysdig` Ansible role under `playbooks/roles/sysdig/file/`.   If you want to change the configmap used by the sysdig-agent, make a copy of the file named `sysdig-agent-configmap.yml`, edit the copy and have the `sysdig_agent_configmap_manifest` variable point to this file. More information can be found on the Sysdig documentation site https://docs.sysdig.com/en/openshift-agent-installation-steps.html

Before you use  the playbook, make sure the current context of the KUBECONFIG file used by Ansible is that of a user with the `cluster-admin` role. By default this file is  `<install_dir>/auth/kubeconfig` where `<install_dir>` is the value specified by the variable of the same name in `group_vars/all/vars.yml` which is usually (but not necessarily) `~/.ocp`.

In the example below, we use the `kubeadmin` user which has enough privileges to deploy sysdig

```
# export KUBECONFIG=~/.ocp/auth/kubeconfig
[core@hpe-ansible gen9cluster]$ oc login -u kubeadmin
Authentication required for https://api.hpe.hpecloud.org:6443 (openshift)
Username: kubeadmin
Password:
Login successful.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[core@hpe-ansible gen9cluster]$
```

Run the playbook

```
# cd <repodir>
# ansible-playbook -i hosts playbooks/sysdig.yml
```

You can then monitor the progresses of the deployment by watching the status of the sysdig-agent project

```
# oc status -n sysdig-agent
```

```
[core@hpe-ansible gen9cluster]$ oc status -n sysdig-agent
In project sysdig-agent on server https://api.hpe.hpecloud.org:6443

daemonset/sysdig-agent manages sysdig/agent
  generation #1 running for 11 minutes - 5 pods


1 info identified, use 'oc status --suggest' to see details.
```

Note: The number of pods deployed by the daemonset should match your number of nodes in your OpenShift cluster.

