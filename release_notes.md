# Release Notes

## Bug Fixes

### Load balancers: firewall zone settings are lost after a reboot

- After a reboot, the two interfaces of the load balancer VMs are now assigned to the correct firewall zones.

### DNS and DHCP services are not automatically restarted after a reboot of a support node

- These services are now started automatically after a reboot.

## OCP 4.2 Support

### Support for OpenShift Container Platform version 4.2

- This version of the playbooks now supports the deployment of OCP 4.2.

### Scaling an OCP 4.2 cluster with Red Hat CoreOS worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).
- When an RHCOS worker node is added to the cluster, OCP 4.2 requires the approval of two Certificate Signing Requests (CSRs).
- The scale.yml playbook has been modified to approve these CSRs when adding RHCOS nodes to the cluster.

### Scaling an OCP 4.2 cluster with Red Hat 7.6 worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).
- Scaling an OCP 4.2 cluster with RHEL worker nodes requires the use of an updated version of the OpenShift Ansible playbooks https://github.com/openshift/openshift-ansible.
- HPE has tested with version 4.2.0-201910111434 of the OpenShift Ansible playbooks and recommends using this version when scaling an OCP 4.2 cluster with RHEL worker nodes.

### Make the master nodes non-schedulable

- By default, the OCP 4.2 installer generates Kubernetes manifests which will make the master nodes schedulable. This was done in an attempt to support clusters with smaller footprints.
- However, due to a limitation with Kubernetes where router Pods running on control plane machines will not be reachable by the ingress load balancer, a manifest generated by the installer needs to be modified prior to running the actual installation in order to make the master nodes non-schedulable.
- According to Red Hat, this may be corrected in a future minor version of OCP.

### EFK Logging Stack

- The process of deploying the EFK (Elasticsearch, Fluentd, Kibana) logging stack has changed slightly in OCP 4.2.
- The efk.yml playbook, and supporting efk role, have been updated to work with OCP 4.2.
- NOTE - This updated version of the EFK playbooks will not work with an OCP 4.1 cluster.

## New Features

### Sysdig Integration
You can now use the playbook `playbooks/sysdig.yml` to integrate your cluster with Sysdig. The implementation in this solution uses the Software as a Service (SaaS) version of Sysdig at [app.sysdigcloud.com](). The playbook deploys the Sysdig Agent software on all OpenShift node. Captured data is relayed back to your Sysdig SaaS Cloud portal.

Here are the variables that you must configure prior to using the `sysdig.yml` playbook.



| Variable                        | File                       | Description                                                  |
| ------------------------------- | -------------------------- | ------------------------------------------------------------ |
| `vault.sysdig_access_key`       | `group_vars/all/vault.yml` | Your Sysdig access key                                       |
| `sysdig_access_key`             | `group_vars/all/vars.yml`  | Value must be `"{{ vault.sysdig_access_key }}"` if you want to encrypt the actual access key (recommended) |
| `sysdig_k8s_cluster_name`       | `group_vars/all/vars.yml`  | Setting cluster name allows you to view, scope, and segment metrics in the Sysdig Monitor UI by Kubernetes cluster. |
| `sysdig_tags`                   | `group_vars/all/vars.yml`  | Optional. Comma separated list of key:value pairs            |
| `sysdig_collector`              | `group_vars/all/vars.yml`  | Optional. Defaults to `collector.sysdigcloud.com` which is the collector to use for the SaaS-based solution by Sysdig. |
| `sysdig_collector_port`         | `group_vars/all/vars.yml`  | Optional. Defaults to 6666                                   |
| `sysdig_ssl`                    | `group_vars/all/vars.yml`  | Optional. Defaults to `True`                                 |
| `sysdig_ssl_verify_certificate` | `group_vars/all/vars.yml`  | Optional. Defaults to `True`                                 |
| `sysdig_new_k8s`                | `group_vars/all/vars.yml`  | Optional. Defaults to `True`. Allows kube state metrics to be automatically detected, monitored, and displayed in Sysdig Monitor. |

Before you use  the playbook, make sure the current context of the KUBECONFIG file used by Ansible is that of a user with the `cluster-admin` role. By default this file is  `<install_dir>/auth/kubeconfig` where `<install_dir>` is the value specified by the variable of the same name in `group_vars/all/vars.yml` which is usually (but not necessarily) `~/.ocp`.

In the example below, we use the `kubeadmin` user which has enough privileges to deploy sysdig

```
# export KUBECONFIG=~/.ocp/auth/kubeconfig
[core@hpe-ansible gen9cluster]$ oc login -u kubeadmin
Authentication required for https://api.hpe.hpecloud.org:6443 (openshift)
Username: kubeadmin
Password:
Login successful.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[core@hpe-ansible gen9cluster]$
```

Run the playbook

```
# cd <repodir>
# ansible-playbook -i hosts playbooks/sysdig.yml
```

You can then monitor the progresses of the deployment by watching the status of the sysdig-agent project

```
# oc status -n sysdig-agent
```

```
[core@hpe-ansible gen9cluster]$ oc status -n sysdig-agent
In project sysdig-agent on server https://api.hpe.hpecloud.org:6443

daemonset/sysdig-agent manages sysdig/agent
  generation #1 running for 11 minutes - 5 pods


1 info identified, use 'oc status --suggest' to see details.
```

Note: The number of pods deployed by the daemonset should match your number of nodes in your OpenShift cluster.

