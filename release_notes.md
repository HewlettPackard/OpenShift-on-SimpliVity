# Release Notes

## Bug Fixes

### Under heavy load, the deployment playbook exits with a timeout if it finds cluster operators which are not "Available"

After a successful bootstrap of the OCP cluster, the deployment playbook (site.yml) enters a loop to verify that all cluster operators are "Available".   After a specific amount of time (10mns), the playbook failed is some of the cluster operators were not "Available". This has been changed to the following:

- The above loop is maintained but the playbook will not fail if the condition for ending the loop is not met before the built-in timer expires.
- Regardless the result of the loop, the OCP installer is called to wait for the end of the installation or until a built-in timer expires (30mns)
- Note that the loop is maintained because is provides feedback to the end user. ON the other hand the OCP installer waits for the end of the installation but  only provides feedback when the installation is finished or when its built-in timer (30mns) expires. without any feedback, the end user might think that the process is stuck in some endless loop or wait for something that will never happen.



### Load balancers: firewall zone settings are lost after a reboot

- After a reboot, the two interfaces of the load balancer VMs are now assigned to the correct firewall zones.

### DNS and DHCP services are not automatically restarted after a reboot of a support node

- These services are now started automatically after a reboot.

## OCP 4.2 Support

### Support for OpenShift Container Platform version 4.2

- This version of the playbooks now supports the deployment of OCP 4.2.

### Scaling an OCP 4.2 cluster with Red Hat CoreOS worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).
- When an RHCOS worker node is added to the cluster, OCP 4.2 requires the approval of two Certificate Signing Requests (CSRs).
- The scale.yml playbook has been modified to approve these CSRs when adding RHCOS nodes to the cluster.

### Scaling an OCP 4.2 cluster with Red Hat 7.6 worker nodes

- OCP 4.2 supports worker/compute nodes running either Red Hat CoreOS (RHCOS) or Red Hat Enterprise Linux 7.6 (RHEL).

- Scaling an OCP 4.2 cluster with RHEL worker nodes requires the use of an updated version of the OpenShift Ansible playbooks https://github.com/openshift/openshift-ansible.

- HPE has tested with release openshift-ansible-4.2.2-201910250432 of the OpenShift Ansible playbooks and recommends using this version when scaling an OCP 4.2 cluster with RHEL worker nodes. Use the following command to clone the openshift-ansible repository:

  ```
  # git clone -b openshift-ansible-4.2.2-201910250432 https://github.com/openshift/openshift-ansible.git
  ```

  

### Make the master nodes non-schedulable

- By default, the OCP 4.2 installer generates Kubernetes manifests which will make the master nodes schedulable. This was done in an attempt to support clusters with smaller footprints.
- However, due to a limitation with Kubernetes where router Pods running on control plane machines will not be reachable by the ingress load balancer, a manifest generated by the installer needs to be modified prior to running the actual installation in order to make the master nodes non-schedulable.
- According to Red Hat, this may be corrected in a future minor version of OCP.

### EFK Logging Stack

- The process of deploying the EFK (Elasticsearch, Fluentd, Kibana) logging stack has changed. This is described in the next section
- The efk.yml playbook, and supporting efk role, have been updated to deploy successfully on either OCP 4.1 or OCP 4.2.

## 



## New Features

### Sysdig Integration

You can now use the playbook `playbooks/sysdig.yml` to integrate your cluster with Sysdig. The implementation in this solution uses the Software as a Service (SaaS) version of Sysdig at [app.sysdigcloud.com](). The playbook deploys the Sysdig Agent software on all OpenShift node. Captured data is relayed back to your Sysdig SaaS Cloud portal.

`Note:` The SaaS version of Sysdig is not supported in environments where HTTP/HTTPS proxy servers are required to access the Internet.

Here are the variables that you must configure prior to using the `sysdig.yml` playbook.



| Variable                        | File                       | Description                                                  |
| ------------------------------- | -------------------------- | ------------------------------------------------------------ |
| `vault.sysdig_access_key`       | `group_vars/all/vault.yml` | Your Sysdig access key                                       |
| `sysdig_access_key`             | `group_vars/all/vars.yml`  | Value must be `"{{ vault.sysdig_access_key }}"` if you want to encrypt the actual access key (recommended) |
| `sysdig_k8s_cluster_name`       | `group_vars/all/vars.yml`  | Setting cluster name allows you to view, scope, and segment metrics in the Sysdig Monitor UI by Kubernetes cluster. |
| `sysdig_tags`                   | `group_vars/all/vars.yml`  | Optional. Comma separated list of key:value pairs            |
| `sysdig_collector`              | `group_vars/all/vars.yml`  | Optional. Defaults to `collector.sysdigcloud.com` which is the collector to use for the SaaS-based solution by Sysdig. |
| `sysdig_collector_port`         | `group_vars/all/vars.yml`  | Optional. Defaults to 6666                                   |
| `sysdig_ssl`                    | `group_vars/all/vars.yml`  | Optional. Defaults to `True`                                 |
| `sysdig_ssl_verify_certificate` | `group_vars/all/vars.yml`  | Optional. Defaults to `True`                                 |
| `sysdig_new_k8s`                | `group_vars/all/vars.yml`  | Optional. Defaults to `True`. Allows kube state metrics to be automatically detected, monitored, and displayed in Sysdig Monitor. |

Before you use  the playbook, make sure the current context of the KUBECONFIG file used by Ansible is that of a user with the `cluster-admin` role. By default this file is  `<install_dir>/auth/kubeconfig` where `<install_dir>` is the value specified by the variable of the same name in `group_vars/all/vars.yml` which is usually (but not necessarily) `~/.ocp`.

In the example below, we use the `kubeadmin` user which has enough privileges to deploy sysdig

```
# export KUBECONFIG=~/.ocp/auth/kubeconfig
[core@hpe-ansible gen9cluster]$ oc login -u kubeadmin
Authentication required for https://api.hpe.hpecloud.org:6443 (openshift)
Username: kubeadmin
Password:
Login successful.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[core@hpe-ansible gen9cluster]$
```

Run the playbook

```
# cd <repodir>
# ansible-playbook -i hosts playbooks/sysdig.yml
```

You can then monitor the progresses of the deployment by watching the status of the sysdig-agent project

```
# oc status -n sysdig-agent
```

```
[core@hpe-ansible gen9cluster]$ oc status -n sysdig-agent
In project sysdig-agent on server https://api.hpe.hpecloud.org:6443

daemonset/sysdig-agent manages sysdig/agent
  generation #1 running for 11 minutes - 5 pods


1 info identified, use 'oc status --suggest' to see details.
```

Note: The number of pods deployed by the daemonset should match your number of nodes in your OpenShift cluster.



### Deployment of the logging Stack

The following variables can be used to deploy the EFK stack. The `#` sign in front of the name of a variable means that the variable can be ommited and the default value will be used. If you want to override the defaults values, specify the variables in `group_vars/all/vars.yml` (without a # sign).

| Variable                   | Default value | Description                                                  |
| -------------------------- | ------------- | ------------------------------------------------------------ |
| `efk_channel`              |               | Mandatory. "preview" when deploying on OCP 4.1, "4.2" when deploying OCP 4.2 |
| #`efk_es_pv_size`          | `200G`        | Size of the persistent volume which will holds the elasticsearch data. |
| #`efk_es_pv_storage_class` | `thin`        | This is the name of the storage class to use for persistent storage. |
| #`efk_profile`             | `small`       | `small` or `large`. the `small` profile deploys a single instance of elasticsearch and a single instance of kibana. There is no redundancy of the elasticsearch data. Also the elasticsearch resource limits are set to 100m core and 2Gi or RAM. |
|                            |               | The `large` profile will deploy 3 elasticsearch instances and 2 kibana instances. The elasticsearch instances require 16Gi of RAM each and 1 CPU core so you will need to deploy 3 large worker nodes. |



1- Populate the group_vars/vars/all.yml file with the above variables if you are not happy with the default values. 

2- Make sure your current `kubeconfig` context is that of a cluster administrator.

```
# export KUBECONFIG ~/.ocp/auth/kubeconfig
# oc login
 (creds of a cluster admin, can be `kubeadmin` if you did not delete the accout)
```

3- Make sure you have enough worker nodes to accommodate the EFK profile you chose. The following characteristics are recommended if you plan to use the `large` profile. Use the `scale.yml` playbook if you need to deploy additional worker nodes.

```
cpus=8 ram=32768
```

4- Once this is done you are ready to run the `efk.yml` playbook:

```
# ansible-playbook -i hosts playbooks/efk.yml
```

The playbook INITIATE the deployment of the EFK stack. You will need to wait for all pods to be up and running before attempting to access the Kibana console. In the example below a large profile was deployed with 3 elasticsearch instances and 2 kibana instance.

```
[core@hpe-ansible gen9cluster]$ oc get pod
NAME                                            READY   STATUS      RESTARTS   AGE
cluster-logging-operator-57b78bf896-tzx8m       1/1     Running     0          54m
curator-1574525400-vb2hp                        0/1     Completed   0          7m39s
elasticsearch-cdm-rhq2scyw-1-6df7f4b488-fnvj5   2/2     Running     0          38m
elasticsearch-cdm-rhq2scyw-2-76dfcf79c6-85x5k   2/2     Running     0          41m
elasticsearch-cdm-rhq2scyw-3-667f9ccd64-8g2mm   2/2     Running     0          41m
fluentd-2v4r7                                   1/1     Running     0          54m
fluentd-4lsnp                                   1/1     Running     0          54m
fluentd-7sp97                                   1/1     Running     0          44m
fluentd-lvs4t                                   1/1     Running     0          54m
fluentd-n26wz                                   1/1     Running     0          54m
fluentd-r4fgv                                   1/1     Running     0          54m
fluentd-t9jrr                                   1/1     Running     0          44m
fluentd-tlclv                                   1/1     Running     0          44m
kibana-84cdbf9cbd-5lkpt                         2/2     Running     0          54m
kibana-84cdbf9cbd-jfrjn                         2/2     Running     0          41m
[core@hpe-ansible gen9cluster]$
```



The Kibana console can be accessed at the "route" displayed by the following command:

```
# oc get route -n openshift-logging
```

```
[core@hpe-ansible gen9cluster]$ oc get route -n openshift-logging
NAME     HOST/PORT                                        PATH   SERVICES   PORT    TERMINATION          WILDCARD
kibana   kibana-openshift-logging.apps.hpe.hpecloud.org          kibana     <all>   reencrypt/Redirect   None
[core@hpe-ansible gen9cluster]$
```

You can then access the Kibana console using the host/port indicated

#### Switching from one EFK profile to another profile

If you want to deploy the large profile, you will need to add additional worker nodes. Each instance of elasticsearch in the large profile requires 16Gi of RAM and 1 CPU core.  Use the `scale.yml` playbook to grow your cluster with additional worker nodes.

To switch from one EFK profile to the other, you need to configure the `efk_profile` variable in `group_vars/all/vars.yml` with the profile you want, either `small` or `large` then re-run the **efk.yml** playbook.

In the example below the small model was deployed as indicated by the number of elasticsearch and kibana pods deployed.

```
[core@hpe-ansible gen9cluster]$ oc get pod -n openshift-logging
NAME                                           READY   STATUS      RESTARTS   AGE
cluster-logging-operator-57b78bf896-hcb9p      1/1     Running     0          19m
curator-1574594400-bq7lk                       0/1     Completed   0          54s
elasticsearch-cdm-hyr75ase-1-858468cb4-mk6s9   2/2     Running     0          18m
fluentd-25rbm                                  1/1     Running     0          18m
fluentd-4cth8                                  1/1     Running     0          18m
fluentd-6d564                                  1/1     Running     0          18m
fluentd-76bd5                                  1/1     Running     0          18m
fluentd-qjtsz                                  1/1     Running     0          18m
kibana-84cdbf9cbd-xvx8l                        2/2     Running     0          18m
```



We can deploy the large model by specifying `efk_profile: large` in `group_vars/vars.yml` or by specifying the variable in the ansible-playbook command line as shown below:

```
 # ansible-playbook -i hosts playbooks/efk.yml -e efk_profile=large
```

After a few seconds the deployment is updated.

```
[core@hpe-ansible gen9cluster]$ oc get pod -n openshift-logging
NAME                                            READY   STATUS      RESTARTS   AGE
cluster-logging-operator-57b78bf896-hcb9p       1/1     Running     0          70m
curator-1574597400-2lddg                        0/1     Completed   0          2m35s
elasticsearch-cdm-hyr75ase-1-858468cb4-mk6s9    2/2     Running     0          70m
elasticsearch-cdm-hyr75ase-2-7954679f69-hrw7r   0/2     Pending     0          50s
elasticsearch-cdm-hyr75ase-3-79d4897685-hxnpj   0/2     Pending     0          49s
fluentd-25rbm                                   1/1     Running     0          70m
fluentd-4cth8                                   1/1     Running     0          70m
fluentd-6d564                                   1/1     Running     0          70m
fluentd-76bd5                                   1/1     Running     0          70m
fluentd-qjtsz                                   1/1     Running     0          70m
kibana-84cdbf9cbd-5swvw                         2/2     Running     0          51s
kibana-84cdbf9cbd-xvx8l                         2/2     Running     0          70m
```

However, the two additional elasticsearch instances remains in the `Pending` state and the `oc describe` command will give the reason:

```
[core@hpe-ansible gen9cluster]$ oc describe pod elasticsearch-cdm-hyr75ase-2-7954679f69-hrw7r -n openshift-logging | tail
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  82s (x5 over 3m59s)  default-scheduler  0/5 nodes are available: 5 Insufficient memory.
```

We deploy 3 additional worker nodes using the playbook `playbooks/scale.yml` with the following characteristics:

```
hpe-worker2   ansible_host=10.15.152.215 cpus=8 ram=32768 
hpe-worker3   ansible_host=10.15.152.216 cpus=8 ram=32768 
hpe-worker4   ansible_host=10.15.152.217 cpus=8 ram=32768 
```

We verify that the new nodes joined the cluster

```
[core@hpe-ansible gen9cluster]$ oc get node
NAME          STATUS   ROLES    AGE    VERSION
hpe-master0   Ready    master   99m    v1.14.6+c07e432da
hpe-master1   Ready    master   99m    v1.14.6+c07e432da
hpe-master2   Ready    master   99m    v1.14.6+c07e432da
hpe-worker0   Ready    worker   99m    v1.14.6+c07e432da
hpe-worker1   Ready    worker   99m    v1.14.6+c07e432da
hpe-worker2   Ready    worker   108s   v1.14.6+c07e432da
hpe-worker3   Ready    worker   91s    v1.14.6+c07e432da
hpe-worker4   Ready    worker   99s    v1.14.6+c07e432da
```

And now verify the EFK deployment

```
[core@hpe-ansible gen9cluster]$ oc get pod -n openshift-logging
NAME                                            READY   STATUS      RESTARTS   AGE
cluster-logging-operator-57b78bf896-hcb9p       1/1     Running     0          84m
curator-1574598000-vdxmv                        0/1     Completed   0          6m17s
elasticsearch-cdm-hyr75ase-1-858468cb4-mk6s9    2/2     Running     0          83m
elasticsearch-cdm-hyr75ase-2-7954679f69-hrw7r   1/2     Running     0          14m
elasticsearch-cdm-hyr75ase-3-79d4897685-hxnpj   1/2     Running     0          14m
fluentd-25rbm                                   1/1     Running     0          83m
fluentd-4cth8                                   1/1     Running     0          83m
fluentd-6d564                                   1/1     Running     0          83m
fluentd-76bd5                                   1/1     Running     0          83m
fluentd-794bl                                   1/1     Running     0          104s
fluentd-8p6jx                                   1/1     Running     0          97s
fluentd-gqrbx                                   1/1     Running     0          95s
fluentd-qjtsz                                   1/1     Running     0          83m
kibana-84cdbf9cbd-5swvw                         2/2     Running     0          14m
kibana-84cdbf9cbd-xvx8l                         2/2     Running     0          83m
```

Of course you may have deployed the large worker nodes prior to redeploying the EFK stack using the large model. This would have worked as well but this workflow was used as it conveys more information on how OpenShift and Kubernetes are working.

### SimpliVity Specific Feature

The playbooks now leverage the SimpliVity API in some areas of the solution. In order to leverage this API you need to configure the following variable in `group_vars/all/vars.yml`:

`simplivity_appliances`: The list of IP address of each OmniStack appliances in the SimpliVity cluster. If you don't configure this variable (or if this list is empty) you disable all SimpliVity related capabilities which is the default.

Strictly speaking, only one IP address is needed (a list with one IP address), however, for HA purposes, HPE recommends you specify the IP address of each OmniStack appliance in your cluster.

Example:

```
simplivity_appliances:
- 10.10.173.109
- 10.10.173.110
- 10.10.173.111
```



### Datastores automatic Provisioning

This feature requires a a SimpliVity cluster which means you need to configure the `simplivity_appliances` variable.  If your cluster is not a SimpliVity cluster you MUST provision the datastores manually.

This solution currently uses up to two datastores:

1. The datastore where all the VMS are landed.  This is the first member of the Ansible variable `datastores`.   Note that only one VM datastore is supported at this time.
2. The datastore designated by the Ansible variable `csi_datastore_name`.  This is the datastore used to store persistent volumes created with the new CSI Storage driver. This can actually be the same datastore as the first one.

The playbook which requires these datastores leverage a new role which will verify that a datastore is present and will create it if this is not the case, with the expected name and the expected size. If the datastores where pre-provisioned, the size of the datastores are left unchanged.

The size of the VM datastore can be controller by the variable `datastore_size` that you should configure in group_vars/all/vars.yml.  The default is to create a 1TB datastore.

The size of the CSI  datastore can also be controlled and this is explained in the next section.



### VMware CSI Storage driver Integration

By default, the vSphere Cloud Provider legacy storage plugin is installed.  if you are running ESX 6.7U3 you may install the vSphere Container Storage Interface Driver.  Installation instructions are provided by VMware [here](https://docs.vmware.com/en/VMware-vSphere/6.7/Cloud-Native-Storage/GUID-039425C1-597F-46FF-8BAA-C5A46FF10E63.html). A playbook (`playbooks/csi.yml`) is provided which will automate these instructions for you.

The following Ansible variables can be configured in `group_vars/all/vars.yml` 

| Variable                 | Description                                                  |
| ------------------------ | ------------------------------------------------------------ |
| `#csi_datastore_name`    | Name of the datastore which will hold the persistent volumes. If this variable is not configured, the first datastore listed in the Ansible variable `datastores` is used. |
| `#csi_datastore_size`    | Size in GiB of the CSI datastore, If the datastore exists, the size is left unchanged. If this variable is not configured and the datastore needs to be created, the size of the datastore will be 512GiB. |
| `#csi_storageclass_name` | Name of the storage class which will be created. If a storage class with the same name is found, it will be left unmodified. If this variable is not configured and the storage class does not exist, the storage class will be configured with the name **csivols** |

To deploy the vSphere  CSI driver:

1. If you want the playbooks to automatically provision the datastore which you will be using as backend storage for persistent volumes configure the `simplivity_appliances` Ansible variable in `group_vars/all/vars.yml`. You need a SimpliVity cluster for this to work.

2. If you don't have a SimpliVity cluster, make sure you don't configure the Ansible variable `simplivity_appliances`. Or make it en empty list. Make sure you provision the datastore manually.

   ```
   simplivity_appliances: []  # do not enable SimpliVity features
   ```

   

3. Configure the `csi_*` variables in `group_vars/all/vars.yml`. This is how you specify the name and size of the datastore which will hold the persistent volumes and the name of the corresponding storage class. We recommend to use a dedicated datastore for this purpose.

4. Make sure you have cluster admin credentials.  The account you use may be the `kubeadmin` account (if you did not already delete it), or any user in your LDAP environment which was granted the `cluster-admin` role.

   ```
   $ oc login -u <adminuser>
   (enter password>
   $ oc whoami
   ```

5. Run the playbook

```
# ansible-playbook -i hosts playbooks/csi.yml
```

It may take a few minutes (2 or 3 typically) for all the CSI pods to be operational (Running). these pods are deployed in the `kube-system` namespace. You can see their status using the following commands

```
[core@hpe-ansible]$ oc get pods -n kube-system
[core@hpe-ansible gen9cluster]$ oc get pods -n kube-system -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
vsphere-csi-controller-0   5/5     Running   0          8h    10.15.152.211   hpe-master1   <none>           <none>
vsphere-csi-node-6t746     3/3     Running   0          8h    10.15.152.213   hpe-worker0   <none>           <none>
vsphere-csi-node-lnhr7     3/3     Running   0          8h    10.15.152.214   hpe-worker1   <none>           <none>

```



You should have one vsphere-csi-node pod running on each worker node.


### Cluster verification (hpe_cluster_verification playbook)

During the solution deployment all components are validated as they are deployed.  Upon completion of the deployment playbooks the OCP cluster should be up, functioning correctly and ready for application deployments. After the OCP cluster is sucessfully deployed using site.yml a final cluster verification playbook is run to execute additional verification tasks.  The roles run as part of this verification playbook are defined in the playbooks/roles/hpe_cluster_verification/defaults/main.yml file.

The hpe_cluster_verification.yml playbook can be manually run, after the OCP cluster has been deployed, using the following command:

   `ansible-playbook -i hosts playbooks/hpe_cluster_verification.yml`

### Sample WordPress application deployment (wordpress.yml playbook)

Included in the OpenShift-on-SimpliVity repository is a playpook that can be used to deploy, validate and remove a sample WordPress application on the OCP cluster.  This sample WordPress application is also used in the hpe_cluster_verification playbook to verify the cluster is ready to deploy user applications.


The sample WordPress application deployment performs the following operations:
1. Deployment
   - Create a namespace for the WordPress application
   - Create a storage class for the WordPress and MySQL for persistent storage
   - Create persistent volume claims (PVC) for WordPress and MySQL
   - Create deployments for WordPress and MySQL
   - Create services for WordPress and MySQL
   - Expose a route to the WordPress application server

2. Validation
   - Validate the MySQL and WordPress PODs are Ready
   - Create an HTTP connection to the WordPress application server

3. Teardown
   - Remove the route exposed to the WordPress application server
   - Remove the services for WordPress and MySQL
   - Remove the WordPress and MySQL deployments
   - Remove PVC's create for WordPress and MySQL
   - Remove storage class created for WordPress and MySQL
   - Remove WordPress application namespace

### Automatically deploying, validating and removing the WordPress application

This playbook is run from the hpe_cluster_verification playbook during initial cluster deployment (site.yml)

### Manually running the wordpress.yml playbook

The wordpress.yml playbook can be manually run using the following command:

   `ansible-playbook -i hosts playbooks/wordpress.yml`

With no options the playbook will deploy, validate and teardown the sample WordPress application.  Each operation can be disabled to run by using the -e option with the ansible-playbook command.  For example:

Only deploy the wordpress application:

   `ansible-playbook  -i hosts playbooks/wordpress.yml -e "teardown=no validate=no"`

Only teardown the application

   `ansible-playbook  -i hosts playbooks/wordpress.yml -e "provision=no validate=no"`

Only run the application validation tasks

   `ansible-playbook  -i hosts playbooks/wordpress.yml -e "teardown=no provision=no"`

Deploy and validate the WordPress application, but do not tear it down

   `ansible-playbook  -i hosts playbooks/wordpress.yml -e "teardown=no"`

Setting/names used for the sample WordPress application are found in the playbooks/roles/wordpress/defaults/main.yml file:

WordPress general settings
```
wp_app_name: 'hpe-wordpress'
wp_proj_name: 'hpe-wordpress-ns'
wp_disp_name: 'HPE WordPress/MySQL validation deployment'
wp_desc: 'HPE Wordpress/MySQL Deployment'
wp_storage_name: 'hpe-wp-storage-class'

```
MySQL settings
```
wp_mysql_pv_claim: 'hpe-mysql-pv-claim'
wp_mysql_svc: 'hpe-mysql-service'
wp_mysql_route: 'hpe-mysql-route'
wp_mysql_deploy: 'hpe-mysql-deploy'
```
WordPress settings
```
wp_wp_pv_claim: 'hpe-wp-pv-claim'
wp_wp_svc: 'hpe-wordpress-service'
wp_wp_route: 'hpe-wordpress-route'
wp_wp_deploy: 'hpe-wp-deploy'
```

=======
### Proxy Support

The playbooks now support deploying the OCP 4.2 solution in environments that require a proxy server to reach the open Internet. The default configuration assumes there is no proxy server in the environment. Both the solution playbook variables and the Ansible node must be properly configured for proxy support.

#### Playbook Variables

Three new variables were added to the `group_vars/all/vars.yml` file to add proxy support:

| Variable                 | Description                                                  |
| ------------------------ | ------------------------------------------------------------ |
| `http_proxy`    | Hostname or IP address of the HTTP proxy server and the proxy port number separated by a colon.  For example: "http://web-proxy.hpecloud.org:8080". |
| `https_proxy`    | Hostname or IP address of the HTTPS proxy server and the proxy port number separated by a colon. Typically this value is set identical to `http_proxy`. For example: "http://web-proxy.hpecloud.org:8080". |
| `no_proxy` | A comma-separated list of hostnames, IP addresses, or network ranges that should bypass the proxy server. By default the list includes: localhost, the configured domain name used to deploy the OCP cluster (`domain_name` variable), the DHCP subnet CIDR (`dhcp_subnet` variable), and the vCenter hostname (`vcenter_hostname` variable). This value can be customized to include additional values required by the customer environment. |

By default all three variables are commented-out, which is the proper configuration in environments where no proxy server is needed to reach the Internet.

#### Ansible Node

The Ansible node should be configured to match the proxy requirements of the customer environment. The only `required` proxy configuration on the Ansible node is to ensure the solution playbooks can install any necessary software packages, such as a local HTTP server. This can be done by either adding a proxy entry to the `/etc/dnf/dnf.conf` file or by setting system-wide proxy settings in the `/etc/environment` file.

An example of configuring a proxy server in the `/etc/dnf/dnf.conf` file is:

```bash
[main]
gpgcheck=1
installonly_limit=3
clean_requirements_on_remove=True
best=False
skip_if_unavailable=True
proxy=http://web-proxy.hpecloud.org:8080
```
